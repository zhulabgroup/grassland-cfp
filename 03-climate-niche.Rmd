# Climate niche estimation

Setup and load packages.

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, eval = TRUE, cache = TRUE)
```

```{r}
library(tidyverse)
library(sf)
library(patchwork)
```

## Get occurrence data

The first question is what the geographic extent that we search for occurrence records. Joise used the state of California for convenience, but it seems to make more sense using an ecological, not administrative, boundary. We thought about using NEON domains. After much discussion, we decided to use the [biodiversity hotspots](https://www.cepf.net/our-work/biodiversity-hotspots). The most relevant one is California Floristic Province (CFP). Study sites seem to distribute nicely across CFP. 

```{r}
site_sf <- read_rds("data/community/site-info.rds")
site_sf %>%
  as_tibble() %>% # drop geometry
  knitr::kable()
```

Make a map overlaying sites with CFP and admin boundaries from [Natural Earth](https://www.naturalearthdata.com/). 

```{r}
cfp_sf <- st_read("data/occurrence/hotspots/hotspots_2016_1.shp") %>%
  filter(
    NAME == "California Floristic Province",
    Type == "hotspot area"
  )

(ggplot() +
  geom_sf(
    data = rnaturalearth::ne_states(
      country = c("Mexico", "United States of America"),
      returnclass = "sf"
    ),
    fill = NA,
    color = alpha("black", .1)
  ) +
  geom_sf(data = cfp_sf, fill = "white", alpha = .5) +
  geom_sf(data = site_sf, color = "red", aes(text = name)) +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(-125, -115), ylim = c(28, 44))
) %>%
  plotly::ggplotly(tooltip = "text")
```

Load all the species from community data.

```{r}
spp_tbl <- read_rds("data/community/all-experimental-data.rds") %>%
  select(species) %>%
  bind_rows(
    read_rds("data/community/all-experimental-data.rds") %>%
      select(species)
  ) %>%
  distinct(species) %>%
  arrange(species)
```

### GBIF

Extract occurrence data from GBIF, using the species list. The query output is an object of class `occdat`. Convert it to nested data, then unnest the data. Save GBIF occurrence data to a file.

```{r, eval=FALSE}
# ~ 13 min to run
library(foreach)
library(doSNOW)
num_cores <- 22 # socs-stats has 44 cores
cl <- makeCluster(num_cores)
registerDoSNOW(cl)
gbif_box_ls <-
  foreach(
    i = 1:length(spp_tbl$species),
    .packages = c("spocc", "sf", "tidyverse")
  ) %dopar% {
    sp <- spp_tbl$species[i]
    res <- occ(
      query = sp, from = "gbif", has_coords = TRUE, limit = 1e6,
      geometry = st_bbox(cfp_sf),
      gbifopts = list(
        # occ_options(from = "gbif", where = "console")
        hasGeospatialIssue = FALSE
      )
    )
    print(i)
    tibble(queryName = sp, gbif = res$gbif$data) %>%
      unnest(cols = c(gbif))
  }
stopCluster(cl)

gbif_box_tbl <- bind_rows(gbif_box_ls) # collapse from list to tibble

gbif_cfp_tbl <- gbif_box_tbl %>%
  select(key, longitude, latitude) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  st_intersection(cfp_sf) %>% # find GBIF and CFP intersection
  as_tibble() %>% # drop geometry
  select(key) %>% # use key to join (fast)
  right_join(gbif_box_tbl, ., by = "key")

write_rds(gbif_cfp_tbl, "data/occurrence/gbif/gbif-cfp-2022-05-02.rds")

rm(list = c("cl", "num_cores", "gbif_box_ls", "gbif_box_tbl", "gbif_cfp_tbl"))
```

Note that GBIF depends on download dates. Here's a test.

```{r, eval=FALSE}
# tested on 20220424

##### Getting today's occ data
sp <- "Achillea millefolium"
res <- occ(
  query = sp, from = "gbif", has_coords = TRUE, limit = 1e6,
  geometry = st_bbox(cfp_sf),
  gbifopts = list(
    hasGeospatialIssue = FALSE
  )
)
sp_df <- tibble(queryName = sp, gbif = res$gbif$data) %>%
  unnest(cols = c(gbif))

gbif_occ_box_df <- sp_df
occ_sf_box <- st_as_sf(
  x = gbif_occ_box_df %>% dplyr::select(key, longitude, latitude),
  coords = c("longitude", "latitude"),
  crs = 4326
)
occ_sf <- st_intersection(occ_sf_box, cfp_sf)
gbif_occ_df <- gbif_occ_box_df %>%
  right_join(as_tibble(occ_sf) %>% dplyr::select(key), by = "key")

gbif_occ_df_today <- gbif_occ_df
write_rds(gbif_occ_df_today, "data/occurrence/gbif/gbif-cfp-2022-04-27-test.rds")
##### Comparing with previous data

# Comparing with 0323 data
gbif_occ_df_0323 <- read_rds("data/occurrence/gbif/gbif-cfp-2022-03-23.rds")
# some record keys seem to be deleted, some added
# in 0323 but not in today
diff1 <- anti_join(gbif_occ_df_0323 %>% filter(queryName == sp), gbif_occ_df_today, by = c("queryName", "key"))
diff1 %>% nrow()
diff1 %>%
  pull(lastCrawled) %>%
  as.Date() %>%
  unique() %>%
  sort()
# in today but not in 0323
diff2 <- anti_join(gbif_occ_df_today, gbif_occ_df_0323 %>% filter(queryName == sp), by = c("queryName", "key"))
diff2 %>% nrow()
diff2 %>%
  pull(lastCrawled) %>%
  as.Date() %>%
  unique() %>%
  sort()

# Comparing with 0418 data
gbif_occ_df_0418 <- read_rds("data/occurrence/gbif/gbif-cfp-2022-04-18.rds")
# no difference in record keys but some records have been re-crawled judging from the crawId
# in 0418 but not in today
diff1 <- anti_join(gbif_occ_df_0418 %>% filter(queryName == sp), gbif_occ_df_today, by = c("queryName", "key", "crawlId"))
diff1 %>% nrow()
diff1 %>%
  pull(lastCrawled) %>%
  as.Date() %>%
  unique() %>%
  sort()
# in today but not in 0418
diff2 <- anti_join(gbif_occ_df_today, gbif_occ_df_0418 %>% filter(queryName == sp), by = c("queryName", "key", "crawlId"))
diff2 %>% nrow()
diff2 %>%
  pull(lastCrawled) %>%
  as.Date() %>%
  unique() %>%
  sort()

# 0425 data do not have this species for some reason?
```

### iNaturalist

Not directly querying the data separately because:

1. Hard limit of 10,000 records
2. Very slow
3. May be blocked if running too many queries in a minute

```{r, eval=FALSE}
res <- occ(
  query = gbif_occ_df_today, from = "inat", has_coords = TRUE, limit = 10000,
  geometry = st_bbox(cfp_sf)
)
```

4. A recommended way to get a lot of data at once is to use the dataset of research grade observations submitted weekly to GBIF https://www.gbif.org/dataset/50c9509d-22c7-4a22-a47d-8c48425ef4a7

```{r, eval=FALSE}
read_rds("data/occurrence/gbif/gbif-cfp-2022-05-02.rds") %>%
  filter(datasetName == "iNaturalist research-grade observations") %>%
  select(species = queryName, longitude, latitude, key) %>%
  write_rds("data/occurrence/inat/inat-cfp-2022-05-02.rds")
```

### BIEN

Retrieve all BIEN data then select those within CFP. Note, the code can't run because of BIEN server error, as of 5/2/2022.

```{r, eval=FALSE}
library(foreach)
library(doSNOW)
num_cores <- 22 # socs-stats has 44 cores
cl <- makeCluster(num_cores)
registerDoSNOW(cl)
bien_df_list <-
  foreach(
    i = 1:length(spp_tbl$species),
    .packages = c("BIEN", "tidyverse")
  ) %dopar% {
    sp <- spp_tbl$species[i]
    res <- BIEN_occurrence_species(species = sp)
    print(i)
    tibble(queryName = sp, res) %>%
      mutate(date_collected = as.Date(date_collected))
  }
bien_all_df <- bind_rows(bien_df_list) %>%
  mutate(key = row_number()) %>%
  drop_na(latitude, longitude)
stopCluster(cl)

bien_all_sf <- st_as_sf(
  x = bien_all_df %>% dplyr::select(key, longitude, latitude),
  coords = c("longitude", "latitude"),
  crs = 4326
)
bien_cfp_sf <- st_intersection(bien_all_sf, cfp_sf)
bien_cfp_df <- bien_all_df %>%
  right_join(as_tibble(bien_cfp_sf) %>% dplyr::select(key), by = "key")
write_rds(bien_all_df, "data/occurrence/bien/bien-all-2022-04-27.rds")
write_rds(bien_cfp_df, "data/occurrence/bien/bien-cfp-2022-04-27.rds")
```

### eJepson

eJepson might be more accurate compared to iNat and GBIF.

Download full dataset at https://www.cch2.org/portal/collections/harvestparams.php
(Searching within the bounding box of 55N-10N, 130W-50W.)

Keep records with species of interest and within CFP.

```{r, eval=FALSE}
cch_all_tbl <- read_csv("data/occurrence/cch/occurrences.csv") %>%
  filter(scientificName %in% spp_tbl$species) %>%
  drop_na(decimalLongitude, decimalLatitude) %>%
  select(
    queryName = scientificName,
    longitude = decimalLongitude,
    latitude = decimalLatitude,
    key = id
  )

cch_cfp_tbl <- cch_all_tbl %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  st_intersection(cfp_sf) %>% # find CCH and CFP intersection
  as_tibble() %>% # drop geometry
  select(key) %>% # use key to join (fast)
  right_join(cch_all_tbl, ., by = "key")

write_rds(cch_all_tbl, "data/occurrence/cch/cch-all-2022-05-02.rds")
write_rds(cch_cfp_tbl, "data/occurrence/cch/cch-cfp-2022-05-02.rds")

rm(list = c("cch_all_tbl", "cch_cfp_tbl"))
```

### Comparisons

```{r, eval=FALSE}
occ_sf_list[["inat"]] <- read_rds("data/occurrence/gbif/gbif-cfp-2022-04-25.rds") %>%
  filter(datasetName == "iNaturalist research-grade observations") %>%
  select(species = queryName, longitude, latitude, key) %>%
  mutate(dataset = "inat") %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
```


Read as sf object.

```{r, eval=FALSE}
occ_sf_list[["bien"]] <- read_rds("data/occurrence/bien/bien-cfp-2022-04-27.rds") %>%
  select(species = queryName, longitude, latitude, key) %>%
  mutate(key = as.character(key)) %>%
  mutate(dataset = "bien") %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
```

Read GBIF occurrence data as a sf object. 

```{r}
gbif_cfp_sf <- read_rds("data/occurrence/gbif/gbif-cfp-2022-05-02-morning.rds") %>%
  select(species = queryName, longitude, latitude, key) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) # WGS84
```

In total, GBIF has `r nrow(gbif_cfp_sf) %>% format(trim = TRUE, scientific = FALSE, big.mark = ",")` records for `r unique(gbif_cfp_sf$species) %>% length() %>% format(trim = TRUE, scientific = FALSE, big.mark = ",")` species. Make a map to show all occurrence locations (randomly sample 10% records to save render time).

```{r}
ggplot() +
  geom_sf(
    data = rnaturalearth::ne_states(
      country = c("Mexico", "United States of America"),
      returnclass = "sf"
    ),
    fill = NA,
    color = alpha("black", .1)
  ) +
  geom_sf(data = cfp_sf, fill = "white", alpha = .5) +
  geom_sf(
    data = gbif_cfp_sf %>% slice_sample(prop = .5), # randomly sample certain proportion, otherwise too slow to render
    color = "black", alpha = .1, size = .1
  ) +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(-125, -115), ylim = c(28, 44))
```

To compare with other occurrence datasets (see below).

```{r, eval=FALSE}
occ_sf_list <- vector(mode = "list")
occ_sf_list[["gbif"]] <- read_rds("data/occurrence/gbif/gbif-cfp-2022-04-25.rds") %>%
  select(species = queryName, longitude, latitude, key) %>%
  mutate(dataset = "gbif") %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) # WGS84
```


Read as sf object.

```{r, eval=FALSE}
occ_sf_list[["cch"]] <- read_rds("data/occurrence/cch/cch-cfp-2022-04-27.rds") %>%
  dplyr::select(species = queryName, longitude, latitude, key) %>%
  mutate(key = as.character(key)) %>%
  mutate(dataset = "cch") %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
```

To compare among data sets, compare sample size.

```{r, eval=FALSE}
occ_sf <- bind_rows(occ_sf_list)
occ_sf %>%
  as_tibble() %>%
  group_by(dataset) %>%
  summarise(
    record_number = n() %>% format(trim = TRUE, scientific = FALSE, big.mark = ","),
    species_number = unique(species) %>% length() %>% format(trim = TRUE, scientific = FALSE, big.mark = ",")
  ) %>%
  knitr::kable()
```

To compare among data sets, make a map to show all occurrence locations (randomly sample 10% records to save render time).

```{r, eval=FALSE}
ggplot() +
  geom_sf(
    data = rnaturalearth::ne_states(
      country = c("Mexico", "United States of America"),
      returnclass = "sf"
    ),
    fill = NA,
    color = alpha("black", .1)
  ) +
  geom_sf(data = cfp_sf, fill = "white", alpha = .5) +
  geom_sf(
    data = occ_sf %>% slice_sample(prop = .1), # randomly sample certain proportion, otherwise too slow to render
    color = "black", alpha = .1, size = .1
  ) +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(-125, -115), ylim = c(28, 44)) +
  facet_wrap(. ~ dataset, nrow = 1)
```

## Get climate data

TODO: try hydrological year

### CHELSA

Download and process [CHELSA climatology data v2.1](https://chelsa-climate.org/downloads/). We used annual data of temperature and precipitation (see [metadata](https://chelsa-climate.org/bioclim/) below). Daily and monthly data are also available.

```{r, echo=FALSE}
tribble(
  ~shortname, ~longname, ~unit, ~scale, ~offset, ~explanation,
  "bio1", "mean annual air temperature", "°C", "0.1", "-273.15", "mean annual daily mean air temperatures averaged over 1 year",
  "bio12", "annual precipitation amount", "kg m^-2^ year^-1^", "0.1", "0", "accumulated precipitation amount over 1 year",
  "vpd_max", "max monthly vapor pressure deficit", "Pa", "0.1", "0", "the highest monthly vapor pressure deficit"
) %>%
  knitr::kable(caption = "CHELSA climatology data used in the project")
```

Note that precipitation unit, kg m^-2^ year^-1^, is equivalent to mm yr^-1^. 

Download data (not running).

```{r, eval=FALSE}
param_list <- c("bio1", "bio12", "vpd_max")
chelsa_path <- "data/climate/chelsa/"
for (param in param_list) {
  url <- paste0("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_", param, "_1981-2010_V.2.1.tif")
  httr::GET(url = url, httr::write_disk(path = paste0(chelsa_path, param, ".tif"), overwrite = TRUE))
  print(param)
}
```

Load raster data using the raster package.

```{r}
climate_list <- vector(mode = "list")

tmean_ras <- raster::raster("data/climate/chelsa/bio1.tif")
ppt_ras <- raster::raster("data/climate/chelsa/bio12.tif")
vpdmax_ras <- raster::raster("data/climate/chelsa/vpd_max.tif")
# raster::proj4string(tmean_ras) # check that projection is WGS84

climate_list[["chelsa"]] <- list(temp = tmean_ras, prcp = ppt_ras, vpd = vpdmax_ras)
```

### Daymet

Download and process Daymet data (not running).

```{r,eval=FALSE}
bbox <- st_bbox(cfp_sf)
param_list <- c("tmax", "tmin", "prcp")
for (param in param_list) {
  daymet_param_path <- paste0("data/climate/daymet/", param, "/")
  if (!dir.exists(daymet_param_path)) {
    dir.create(daymet_param_path, recursive = T)
  }
  for (year in 1980:2020) {
    # url<-paste0("https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/1840/daymet_v4_daily_na_",param,"_",year,".nc") # daily data are probably too much. takes too long to download and process.
    if (param == "prcp") {
      url <- paste0("https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/1855/daymet_v4_", param, "_monttl_na_", year, ".nc")
    }
    if (param %in% c("tmax", "tmin")) {
      url <- paste0("https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/1855/daymet_v4_", param, "_monavg_na_", year, ".nc")
    }
    httr::GET(url = url, httr::write_disk(path = paste0(daymet_param_path, year, ".nc"), overwrite = TRUE))
    print(paste0(param, ",", year))
  }
}
for (param in param_list) {
  daymet_param_path <- paste0("data/climate/daymet/", param, "/")
  files <- list.files(daymet_param_path, full.names = T)
  for (i in 1:length(files)) {
    file <- files[i]
    bri <- raster::brick(file)
    # potentially do other calculations
  }
}
```

### PRISM

Download and process PRISM using the prism package (not running).

```{r, eval=F}
prism::prism_set_dl_dir("data/climate/prism/")
prism::get_prism_normals("tmean", "800m", annual = TRUE, keepZip = FALSE)
prism::get_prism_normals("ppt", "800m", annual = TRUE, keepZip = FALSE)
prism::get_prism_normals("vpdmax", "800m", annual = TRUE, keepZip = FALSE)
```

Load raster data using the raster package.

```{r, eval=T}
prism::prism_set_dl_dir("data/climate/prism/")
tmean_ras <- prism::prism_archive_subset("tmean", "annual normals", resolution = "800m") %>%
  prism::pd_to_file() %>%
  raster::raster()
ppt_ras <- prism::prism_archive_subset("ppt", "annual normals", resolution = "800m") %>%
  prism::pd_to_file() %>%
  raster::raster()
vpdmax_ras <- prism::prism_archive_subset("vpdmax", "annual normals", resolution = "800m") %>%
  prism::pd_to_file() %>%
  raster::raster()
climate_list[["prism"]] <- list(temp = tmean_ras, prcp = ppt_ras, vpd = vpdmax_ras)
```

## Extract data at occurrence sites

Use GBIF occurrence data longitude and latitude to extract site climate values from CHELSA and PRISM rasters. But first, convert GBIF coordinates in WGS84 to the projection of CHELSA and PRISM rasters.

```{r, eval=F}
occ_clim_sf_list <- vector(mode = "list")
for (clim_dat in c("chelsa", "prism")) {
  occ_sf_reproj <- occ_sf %>%
    filter(dataset == "gbif") %>%
    st_transform(crs = st_crs(climate_list[[clim_dat]][[1]])) # WGS84 for CHELSA, NAD83 for PRISM

  occ_clim_sf <- occ_sf_reproj %>%
    mutate(
      tmean = raster::extract(climate_list[[clim_dat]]$temp, occ_sf_reproj),
      ppt = raster::extract(climate_list[[clim_dat]]$prcp, occ_sf_reproj),
      vpdmax = raster::extract(climate_list[[clim_dat]]$vpd, occ_sf_reproj),
      dataset = clim_dat
    )
  occ_clim_sf_list[[clim_dat]] <- occ_clim_sf
}
occ_clim_sf <- bind_rows(occ_clim_sf_list)
write_rds(occ_clim_sf, "data/climate/occurrence_with_climate_data.rds")
```

Check correlation between CHELSA and PRISM data.
```{r}
occ_clim_sf <- read_rds("data/climate/occurrence_with_climate_data.rds")
ggplot(occ_clim_sf %>%
  as_tibble() %>%
  dplyr::select(tmean, ppt, vpdmax, dataset) %>%
  group_by(dataset) %>%
  mutate(id = row_number()) %>%
  ungroup() %>%
  gather(key = "var", value = "value", -dataset, -id) %>%
  spread(key = "dataset", value = "value") %>%
  mutate(chelsa = case_when(
    var == "vpdmax" ~ chelsa / 100,
    TRUE ~ chelsa
  ))) +
  stat_bin_hex(aes(x = chelsa, y = prism), bins = 100) +
  geom_smooth(aes(x = chelsa, y = prism), method = "lm") +
  geom_abline(intercept = 0, slope = 1, col = "red") +
  facet_wrap(. ~ var) +
  viridis::scale_fill_viridis() +
  theme_classic()
```
```{r}
tmean_chelsa <- occ_clim_sf %>%
  as_tibble() %>%
  filter(dataset == "chelsa") %>%
  pull(tmean)
tmean_prism <- occ_clim_sf %>%
  as_tibble() %>%
  filter(dataset == "prism") %>%
  pull(tmean)
cor.test(tmean_chelsa, tmean_prism)

ppt_chelsa <- occ_clim_sf %>%
  as_tibble() %>%
  filter(dataset == "chelsa") %>%
  pull(ppt)
ppt_prism <- occ_clim_sf %>%
  as_tibble() %>%
  filter(dataset == "prism") %>%
  pull(ppt)
cor.test(ppt_chelsa, ppt_prism)

vpdmax_chelsa <- occ_clim_sf %>%
  as_tibble() %>%
  filter(dataset == "chelsa") %>%
  pull(vpdmax)
vpdmax_prism <- occ_clim_sf %>%
  as_tibble() %>%
  filter(dataset == "prism") %>%
  pull(vpdmax)
cor.test(vpdmax_chelsa, vpdmax_prism)
```


## Quantify species climate niches

### Compare niche estimation using all vs. CFP observations
```{r}
bien_all_sf <- read_rds("data/occurrence/bien/bien-all-2022-04-27.rds") %>%
  dplyr::select(species = queryName, longitude, latitude, key) %>%
  mutate(key = as.character(key)) %>%
  mutate(range = "all") %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
bien_cfp_sf <- read_rds("data/occurrence/bien/bien-cfp-2022-04-27.rds") %>%
  dplyr::select(species = queryName, longitude, latitude, key) %>%
  mutate(key = as.character(key)) %>%
  mutate(range = "cfp") %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
range_compare_sf <- bind_rows(bien_all_sf, bien_cfp_sf)

ggplot(map_data("county", c("california", "nevada", "oregon"))) +
  geom_polygon(aes(long, lat, group = group),
    fill = "white", color = "grey75"
  ) +
  geom_sf(data = range_compare_sf, alpha = .5) +
  geom_sf(
    data = occ_sf %>% slice_sample(n = 1000), # randomly sample 1000, otherwise too slow to render
    color = "black", alpha = .1, size = .1
  ) +
  labs(x = "Longitude", y = "Latitude") +
  facet_wrap(. ~ range, nrow = 1)
```

```{r, eval=F}
range_compare_sf_reproj <- range_compare_sf %>%
  st_transform(crs = st_crs(climate_list[["chelsa"]][[1]])) # WGS84 for CHELSA, NAD83 for PRISM

range_compare_clim_sf <- range_compare_sf_reproj %>%
  mutate(
    tmean = raster::extract(climate_list[["chelsa"]]$temp, range_compare_sf_reproj),
    ppt = raster::extract(climate_list[["chelsa"]]$prcp, range_compare_sf_reproj),
    vpdmax = raster::extract(climate_list[["chelsa"]]$vpd, range_compare_sf_reproj)
  )

write_rds(range_compare_clim_sf, "data/climate/range_comparison_with_climate_data.rds")
```

Compare quantiles of three climatic variables using data from full range and CFP.
```{r}
range_compare_clim_summary <- read_rds("data/climate/range_comparison_with_climate_data.rds") %>%
  as_tibble() %>%
  dplyr::select(species, range, tmean, ppt, vpdmax) %>%
  gather(key = "var", value = "value", -species, -range) %>%
  group_by(species, range, var) %>%
  summarise(
    q5 = quantile(value, 0.05, na.rm = T),
    q25 = quantile(value, 0.25, na.rm = T),
    q50 = quantile(value, 0.5, na.rm = T),
    q75 = quantile(value, 0.75, na.rm = T),
    q95 = quantile(value, 0.95, na.rm = T)
  ) %>%
  gather(key = "quantile", value = "value", -species, -range, -var) %>%
  spread(key = "range", value = "value")

ggplot(range_compare_clim_summary) +
  geom_point(aes(x = all, y = cfp), alpha = 0.5) +
  facet_wrap(. ~ var * quantile, scales = "free", ncol = 5) +
  geom_abline(intercept = 0, slope = 1, col = "red") +
  theme_classic()
```

### Individual species

Visualize individual species occurrence in climate space.

```{r, eval=FALSE}
occ_gbif_sf <- occ_sf %>% filter(dataset == "gbif")
# gbif summary
tmean_rng <- range(occ_gbif_sf$tmean)
ppt_rng <- range(occ_gbif_sf$ppt)
n_occ_tot <- nrow(occ_gbif_sf)
# exp data summary
exp_tbl <- read_rds("data/community/all-experimental-data.rds")
n_exp_tot <- nrow(exp_tbl)
# obs data summary
obs_tbl <- read_rds("data/community/all-observational-data.rds")
n_obs_tot <- nrow(obs_tbl)
niche_gg <- vector(mode = "list")

for (i in 1:length(unique(occ_gbif_sf$species))) {
  sp <- unique(occ_gbif_sf$species)[i]
  occ_sp_sf <- filter(occ_gbif_sf, species == sp)

  occ_sp_stat <- occ_sp_sf %>%
    as_tibble() %>%
    summarise(
      occ_n = n(),
      tmean_occ_mean = mean(tmean, na.rm = TRUE),
      tmean_occ_sd = sd(tmean, na.rm = TRUE),
      ppt_occ_mean = mean(ppt, na.rm = TRUE),
      ppt_occ_sd = sd(ppt, na.rm = TRUE)
    )
  n_exp_sp <- exp_tbl %>%
    filter(species == sp) %>%
    nrow()
  n_obs_sp <- obs_tbl %>%
    filter(species == sp) %>%
    nrow()

  occ_geog <- ggplot(map_data("county", c("california", "nevada", "oregon"))) +
    geom_polygon(aes(long, lat, group = group),
      fill = "white", color = "grey75"
    ) +
    geom_sf(data = cfp_sf, alpha = .5) +
    geom_sf(data = occ_sp_sf, alpha = .1, size = .5) +
    labs(x = "Longitude", y = "Latitude", title = sp) +
    theme(plot.title = element_text(face = "italic"))

  occ_clim <- ggplot(occ_sp_sf, aes(tmean, ppt)) +
    geom_point(alpha = .5, size = .5) +
    geom_rug(alpha = .5) +
    stat_ellipse(col = "red") +
    geom_point(
      data = occ_sp_stat,
      aes(x = tmean_occ_mean, y = ppt_occ_mean),
      shape = 3, col = "red", size = 10
    ) +
    lims(x = tmean_rng, y = ppt_rng) +
    # scale_y_log10() + # log scale ppt
    labs(
      x = "Mean annual temperature (°C)", y = "Annual precipitation (mm)",
      title = str_c(
        "n_occ = ", occ_sp_stat$occ_n, " (", round(occ_sp_stat$occ_n / n_occ_tot * 100, digits = 3), "%)\n",
        "n_exp = ", n_exp_sp, " (", round(n_exp_sp / n_exp_tot * 100, digits = 3), "%)\n",
        "n_obs = ", n_obs_sp, " (", round(n_obs_sp / n_obs_tot * 100, digits = 3), "%)"
      )
    )

  print(sp)
  niche_gg[[i]] <- occ_geog + occ_clim # no need to print(); will slow down
}

names(niche_gg) <- unique(occ_gbif_sf$species)
```

Print all species into a multi-page PDF.

```{r, eval=FALSE}
# runtime ~= 4 min
pdf("figures/species-climate-niche-2022-04-25.pdf", width = 8, height = 8 * .618)
print(niche_gg)
dev.off()
```

### All species

Summarize species climate niches (mean, sd, etc.).

```{r}
niche_tbl <- occ_clim_sf %>%
  as_tibble() %>% # drop sf geometry--too slow and not needed for this summary
  group_by(species) %>%
  summarize(
    occ_n = n(),
    tmean_occ_mean = mean(tmean, na.rm = TRUE),
    tmean_occ_median = median(tmean, na.rm = TRUE),
    tmean_occ_lwr = quantile(tmean, .05, na.rm = TRUE),
    tmean_occ_upr = quantile(tmean, .95, na.rm = TRUE), # hot limit, suggested by Susan Harrison
    tmean_occ_sd = sd(tmean, na.rm = TRUE),
    ppt_occ_mean = mean(ppt, na.rm = TRUE),
    ppt_occ_median = median(ppt, na.rm = TRUE),
    ppt_occ_lwr = quantile(ppt, .05, na.rm = TRUE), # dry limit, suggested by Susan Harrison
    ppt_occ_upr = quantile(ppt, .95, na.rm = TRUE),
    ppt_occ_sd = sd(ppt, na.rm = TRUE),
    vpdmax_occ_mean = mean(vpdmax, na.rm = TRUE),
    vpdmax_occ_median = median(vpdmax, na.rm = TRUE),
    vpdmax_occ_lwr = quantile(vpdmax, .05, na.rm = TRUE),
    vpdmax_occ_upr = quantile(vpdmax, .95, na.rm = TRUE),
    vpdmax_occ_sd = sd(vpdmax, na.rm = TRUE),
  )
```

These different summary statistics (mean, median, upper, and lower quantiles) are highly and significantly correlated.

```{r fig.cap="Species temperature niche statistics (°C)"}
niche_tbl %>%
  select(tmean_occ_mean, tmean_occ_median, tmean_occ_lwr, tmean_occ_upr) %>%
  GGally::ggpairs(
    title = "Species temperature niche (°C)",
    columnLabels = c("Mean", "Median", "Lower limit (5%)", "Upper limit (95%)"),
  )
```

```{r fig.cap="Species precipitation niche statistics (mm)"}
niche_tbl %>%
  select(ppt_occ_mean, ppt_occ_median, ppt_occ_lwr, ppt_occ_upr) %>%
  GGally::ggpairs(
    title = "Species precipitation niche (mm)",
    columnLabels = c("Mean", "Median", "Lower limit (5%)", "Upper limit (95%)")
  )
```

```{r fig.cap="Species VPDmax niche statistics (Pa)"}
niche_tbl %>%
  select(vpdmax_occ_mean, vpdmax_occ_median, vpdmax_occ_lwr, vpdmax_occ_upr) %>%
  GGally::ggpairs(
    title = "Species VPDmax niche (Pa)",
    columnLabels = c("Mean", "Median", "Lower limit (5%)", "Upper limit (95%)")
  )
```

Different dimensions of the niche space (temperature, precipitation, and VPD max) are also correlated (using mean as summary).

```{r fig.cap="Correlation of species niche dimensions"}
niche_tbl %>%
  select(tmean_occ_mean, ppt_occ_mean, vpdmax_occ_mean) %>%
  GGally::ggpairs(
    title = "Species niche dimensions",
    columnLabels = c("Temperature (°C)", "Precipitation (mm)", "VPDmax (Pa)")
  )
```

With all the correlations, we can safely use temperature and precipitation to visualize species niches in climate space. Note the strong negative correlation between temperature and precipitation (r = `r with(niche_tbl, cor(tmean_occ_mean, ppt_occ_mean)) %>% format(digits = 3)`, p = `r with(niche_tbl, cor.test(tmean_occ_mean, ppt_occ_mean))$p.value %>% format(digits = 3)`).

```{r fig.cap="Species climate niches, with mean (point) and standard error (crossbar)"}
(niche_tbl %>%
  ggplot(aes(text = species)) +
  geom_point(aes(tmean_occ_mean, ppt_occ_mean), alpha = .5) +
  geom_errorbarh(aes(xmin = tmean_occ_mean - tmean_occ_sd / sqrt(occ_n), xmax = tmean_occ_mean + tmean_occ_sd / sqrt(occ_n), y = ppt_occ_mean), alpha = .3) +
  geom_errorbar(aes(x = tmean_occ_mean, ymin = ppt_occ_mean - ppt_occ_sd / sqrt(occ_n), ymax = ppt_occ_mean + ppt_occ_sd / sqrt(occ_n)), alpha = .3) +
  labs(
    x = "Species occurrence mean temperature (°C)",
    y = "Species occurrence mean precipitation (mm)"
  )
) %>%
  plotly::ggplotly(tooltip = "text")
```

This figure compares well with Josie's estimates. 

```{r, fig.cap="Species climate niches (mean and standard error) estimated by Josie"}
(read_csv("data/archives/CAGrasslandCommunityChange-master/Analysis/CTI Data/AllSite_CTI_08_2020.csv") %>%
  filter(
    species.name != "Linanthus jepsonii",
    species.name != "Linanthus latisectus",
    relcov >= 0
  ) %>%
  dplyr::select(species = species.name, guild, tmp = meanT, ppt = meanp) %>%
  distinct() %>%
  ggplot(aes(tmp, ppt, text = species)) +
  geom_point(alpha = .5) +
  # geom_smooth(method = "lm", aes(tmp, ppt)) +
  labs(
    x = "Species occurrence mean temperature (°C)",
    y = "Species occurrence mean precipitation (mm)"
  )
) %>%
  plotly::ggplotly(tooltip = "text")
```

Save niche estimates with mean and SD statistics.

```{r}
niche_tbl %>%
  select(species, occ_n, tmean_occ_mean, tmean_occ_sd, ppt_occ_mean, ppt_occ_sd, vpdmax_occ_mean, vpdmax_occ_sd) %>%
  write_rds("data/occurrence/niche-estimates-cfp-2022-04-27.rds")
```
